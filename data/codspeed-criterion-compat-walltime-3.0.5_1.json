{
    "level": "Info",
    "analyzer": "UnsafeDataflow",
    "op_type": "Transmute",
    "description": "Potential unsafe dataflow issue in `analysis::common`",
    "file": "codspeed-criterion-compat-walltime-3.0.5/src/analysis/mod.rs",
    "start_line": 39,
    "start_col": 1,
    "end_line": 263,
    "end_col": 2,
    "code_snippet": "pub(crate) fn common<M: Measurement, T: ?Sized>(\n    id: &BenchmarkId,\n    routine: &mut dyn Routine<M, T>,\n    config: &BenchmarkConfig,\n    criterion: &Criterion<M>,\n    report_context: &ReportContext,\n    parameter: &T,\n    throughput: Option<Throughput>,\n) {\n    criterion.report.benchmark_start(id, report_context);\n\n    if let Baseline::CompareStrict = criterion.baseline {\n        if !base_dir_exists(\n            id,\n            &criterion.baseline_directory,\n            &criterion.output_directory,\n        ) {\n            panic!(\n                \"Baseline '{base}' must exist before comparison is allowed; try --save-baseline {base}\",\n                base=criterion.baseline_directory,\n            );\n        }\n    }\n\n    let (sampling_mode, iters, times);\n    if let Some(baseline) = &criterion.load_baseline {\n        let mut sample_path = criterion.output_directory.clone();\n        sample_path.push(id.as_directory_name());\n        sample_path.push(baseline);\n        sample_path.push(\"sample.json\");\n        let loaded = fs::load::<SavedSample, _>(&sample_path);\n\n        match loaded {\n            Err(err) => panic!(\n                \"Baseline '{base}' must exist before it can be loaded; try --save-baseline {base}. Error: {err}\",\n                base = baseline, err = err\n            ),\n            Ok(samples) => {\n                sampling_mode = samples.sampling_mode;\n                iters = samples.iters.into_boxed_slice();\n                times = samples.times.into_boxed_slice();\n            }\n        }\n    } else {\n        let sample = routine.sample(\n            &criterion.measurement,\n            id,\n            config,\n            criterion,\n            report_context,\n            parameter,\n        );\n        sampling_mode = sample.0;\n        iters = sample.1;\n        times = sample.2;\n\n        if let Some(conn) = &criterion.connection {\n            conn.send(&OutgoingMessage::MeasurementComplete {\n                id: id.into(),\n                iters: &iters,\n                times: &times,\n                plot_config: (&report_context.plot_config).into(),\n                sampling_method: sampling_mode.into(),\n                benchmark_config: config.into(),\n            })\n            .unwrap();\n\n            conn.serve_value_formatter(criterion.measurement.formatter())\n                .unwrap();\n            return;\n        }\n    }\n\n    criterion.report.analysis(id, report_context);\n\n    if times.iter().any(|&f| f == 0.0) {\n        error!(\n            \"At least one measurement of benchmark {} took zero time per \\\n            iteration. This should not be possible. If using iter_custom, please verify \\\n            that your routine is correctly measured.\",\n            id.as_title()\n        );\n        return;\n    }\n\n    let avg_times = iters\n        .iter()\n        .zip(times.iter())\n        .map(|(&iters, &elapsed)| elapsed / iters)\n        .collect::<Vec<f64>>();\n    let avg_times = Sample::new(&avg_times);\n\n    if criterion.should_save_baseline() {\n        log_if_err!({\n            let mut new_dir = criterion.output_directory.clone();\n            new_dir.push(id.as_directory_name());\n            new_dir.push(\"new\");\n            fs::mkdirp(&new_dir)\n        });\n    }\n\n    let data = Data::new(&iters, &times);\n    let labeled_sample = tukey::classify(avg_times);\n    if criterion.should_save_baseline() {\n        log_if_err!({\n            let mut tukey_file = criterion.output_directory.to_owned();\n            tukey_file.push(id.as_directory_name());\n            tukey_file.push(\"new\");\n            tukey_file.push(\"tukey.json\");\n            fs::save(&labeled_sample.fences(), &tukey_file)\n        });\n    }\n    let (mut distributions, mut estimates) = estimates(avg_times, config);\n    if sampling_mode.is_linear() {\n        let (distribution, slope) = regression(&data, config);\n\n        estimates.slope = Some(slope);\n        distributions.slope = Some(distribution);\n    }\n\n    if criterion.should_save_baseline() {\n        log_if_err!({\n            let mut sample_file = criterion.output_directory.clone();\n            sample_file.push(id.as_directory_name());\n            sample_file.push(\"new\");\n            sample_file.push(\"sample.json\");\n            fs::save(\n                &SavedSample {\n                    sampling_mode,\n                    iters: data.x().as_ref().to_vec(),\n                    times: data.y().as_ref().to_vec(),\n                },\n                &sample_file,\n            )\n        });\n        log_if_err!({\n            let mut estimates_file = criterion.output_directory.clone();\n            estimates_file.push(id.as_directory_name());\n            estimates_file.push(\"new\");\n            estimates_file.push(\"estimates.json\");\n            fs::save(&estimates, &estimates_file)\n        });\n    }\n\n    let compare_data = if base_dir_exists(\n        id,\n        &criterion.baseline_directory,\n        &criterion.output_directory,\n    ) {\n        let result = compare::common(id, avg_times, config, criterion);\n        match result {\n            Ok((\n                t_value,\n                t_distribution,\n                relative_estimates,\n                relative_distributions,\n                base_iter_counts,\n                base_sample_times,\n                base_avg_times,\n                base_estimates,\n            )) => {\n                let p_value = t_distribution.p_value(t_value, &Tails::Two);\n                Some(crate::report::ComparisonData {\n                    p_value,\n                    t_distribution,\n                    t_value,\n                    relative_estimates,\n                    relative_distributions,\n                    significance_threshold: config.significance_level,\n                    noise_threshold: config.noise_threshold,\n                    base_iter_counts,\n                    base_sample_times,\n                    base_avg_times,\n                    base_estimates,\n                })\n            }\n            Err(e) => {\n                crate::error::log_error(&e);\n                None\n            }\n        }\n    } else {\n        None\n    };\n\n    let measurement_data = crate::report::MeasurementData {\n        data: Data::new(&iters, &times),\n        avg_times: labeled_sample,\n        absolute_estimates: estimates,\n        distributions,\n        comparison: compare_data,\n        throughput,\n    };\n\n    criterion.report.measurement_complete(\n        id,\n        report_context,\n        &measurement_data,\n        criterion.measurement.formatter(),\n    );\n\n    if criterion.should_save_baseline() {\n        log_if_err!({\n            let mut benchmark_file = criterion.output_directory.clone();\n            benchmark_file.push(id.as_directory_name());\n            benchmark_file.push(\"new\");\n            benchmark_file.push(\"benchmark.json\");\n            fs::save(&id, &benchmark_file)\n        });\n    }\n\n    if criterion.connection.is_none() {\n        if let Baseline::Save = criterion.baseline {\n            copy_new_dir_to_base(\n                id.as_directory_name(),\n                &criterion.baseline_directory,\n                &criterion.output_directory,\n            );\n        }\n    }\n\n    if criterion.should_save_baseline() && ::codspeed::utils::running_with_codspeed_runner() {\n        codspeed::collect_walltime_results(id, criterion, &iters, &times);\n    }\n}"
}
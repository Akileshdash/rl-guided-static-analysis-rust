{
    "level": "Info",
    "analyzer": "UnsafeDataflow",
    "op_type": "Transmute",
    "description": "Potential unsafe dataflow issue in `interned::IngredientImpl::<C>::intern_id`",
    "file": "salsa-0.23.0/src/interned.rs",
    "start_line": 321,
    "start_col": 5,
    "end_line": 566,
    "end_col": 6,
    "code_snippet": "pub fn intern_id<'db, Key>(\n        &'db self,\n        db: &'db dyn crate::Database,\n        key: Key,\n        assemble: impl FnOnce(Id, Key) -> C::Fields<'db>,\n    ) -> crate::Id\n    where\n        Key: Hash,\n        // We'd want the following predicate, but this currently implies `'static` due to a rustc\n        // bug\n        // for<'db> C::Data<'db>: HashEqLike<Key>,\n        // so instead we go with this and transmute the lifetime in the `eq` closure\n        C::Fields<'db>: HashEqLike<Key>,\n    {\n        let (zalsa, zalsa_local) = db.zalsas();\n\n        // Record the current revision as active.\n        let current_revision = zalsa.current_revision();\n        self.revision_queue.record(current_revision);\n\n        // Hash the value before acquiring the lock.\n        let hash = self.hasher.hash_one(&key);\n\n        let shard_index = self.shard(hash);\n        // SAFETY: `shard_index` is guaranteed to be in-bounds for `self.shards`.\n        let shard = unsafe { &mut *self.shards.get_unchecked(shard_index).lock() };\n\n        let found_value = Cell::new(None);\n        // SAFETY: We hold the lock for the shard containing the value.\n        let eq = |id: &_| unsafe { Self::value_eq(*id, &key, zalsa, &found_value) };\n\n        // Attempt a fast-path lookup of already interned data.\n        if let Some(&id) = shard.key_map.find(hash, eq) {\n            let value = found_value\n                .get()\n                .expect(\"found the interned value, so `found_value` should be set\");\n\n            let index = self.database_key_index(id);\n\n            // SAFETY: We hold the lock for the shard containing the value.\n            let value_shared = unsafe { &mut *value.shared.get() };\n\n            // Validate the value in this revision to avoid reuse.\n            if { value_shared.last_interned_at } < current_revision {\n                value_shared.last_interned_at = current_revision;\n\n                zalsa.event(&|| {\n                    Event::new(EventKind::DidValidateInternedValue {\n                        key: index,\n                        revision: current_revision,\n                    })\n                });\n\n                if value_shared.is_reusable::<C>() {\n                    // Move the value to the front of the LRU list.\n                    //\n                    // SAFETY: We hold the lock for the shard containing the value, and `value` is\n                    // a reusable value that was previously interned, so is in the list.\n                    unsafe { shard.lru.cursor_mut_from_ptr(value).remove() };\n\n                    // SAFETY: The value pointer is valid for the lifetime of the database\n                    // and never accessed mutably directly.\n                    unsafe { shard.lru.push_front(UnsafeRef::from_raw(value)) };\n                }\n            }\n\n            if let Some((_, stamp)) = zalsa_local.active_query() {\n                let was_reusable = value_shared.is_reusable::<C>();\n\n                // Record the maximum durability across all queries that intern this value.\n                value_shared.durability = std::cmp::max(value_shared.durability, stamp.durability);\n\n                // If the value is no longer reusable, i.e. the durability increased, remove it\n                // from the LRU.\n                if was_reusable && !value_shared.is_reusable::<C>() {\n                    // SAFETY: We hold the lock for the shard containing the value, and `value`\n                    // was previously reusable, so is in the list.\n                    unsafe { shard.lru.cursor_mut_from_ptr(value).remove() };\n                }\n            }\n\n            // Record a dependency on the value.\n            //\n            // See `intern_id_cold` for why we need to use `current_revision` here. Note that just\n            // because this value was previously interned does not mean it was previously interned\n            // by *our query*, so the same considerations apply.\n            zalsa_local.report_tracked_read_simple(\n                index,\n                value_shared.durability,\n                current_revision,\n            );\n\n            return value_shared.id;\n        }\n\n        // Fill up the table for the first few revisions without attempting garbage collection.\n        if !self.revision_queue.is_primed() {\n            return self.intern_id_cold(\n                db,\n                key,\n                zalsa,\n                zalsa_local,\n                assemble,\n                shard,\n                shard_index,\n                hash,\n            );\n        }\n\n        // Otherwise, try to reuse a stale slot.\n        let mut cursor = shard.lru.back_mut();\n\n        while let Some(value) = cursor.get() {\n            // SAFETY: We hold the lock for the shard containing the value.\n            let value_shared = unsafe { &mut *value.shared.get() };\n\n            // The value must not have been read in the current revision to be collected\n            // soundly, but we also do not want to collect values that have been read recently.\n            //\n            // Note that the list is sorted by LRU, so if the tail of the list is not stale, we\n            // will not find any stale slots.\n            if !self.revision_queue.is_stale(value_shared.last_interned_at) {\n                break;\n            }\n\n            // We should never reuse a value that was accessed in the current revision.\n            debug_assert!({ value_shared.last_interned_at } < current_revision);\n\n            // Record the durability of the current query on the interned value.\n            let (durability, last_interned_at) = zalsa_local\n                .active_query()\n                .map(|(_, stamp)| (stamp.durability, current_revision))\n                // If there is no active query this durability does not actually matter.\n                // `last_interned_at` needs to be `Revision::MAX`, see the `intern_access_in_different_revision` test.\n                .unwrap_or((Durability::MAX, Revision::max()));\n\n            let old_id = value_shared.id;\n\n            // Increment the generation of the ID, as if we allocated a new slot.\n            //\n            // If the ID is at its maximum generation, we are forced to leak the slot.\n            let Some(new_id) = value_shared.id.next_generation() else {\n                // Remove the value from the LRU list as we will never be able to\n                // collect it.\n                cursor.remove().unwrap();\n\n                // Retry with the previous element.\n                cursor = shard.lru.back_mut();\n\n                continue;\n            };\n\n            // Mark the slot as reused.\n            *value_shared = ValueShared {\n                id: new_id,\n                durability,\n                last_interned_at,\n            };\n\n            let index = self.database_key_index(value_shared.id);\n\n            // Record a dependency on the new value.\n            //\n            // See `intern_id_cold` for why we need to use `current_revision` here.\n            zalsa_local.report_tracked_read_simple(\n                index,\n                value_shared.durability,\n                current_revision,\n            );\n\n            zalsa.event(&|| {\n                Event::new(EventKind::DidReuseInternedValue {\n                    key: index,\n                    revision: current_revision,\n                })\n            });\n\n            // Remove the value from the LRU list.\n            //\n            // SAFETY: The value pointer is valid for the lifetime of the database.\n            let value = unsafe { &*UnsafeRef::into_raw(cursor.remove().unwrap()) };\n\n            // SAFETY: We hold the lock for the shard containing the value, and the\n            // value has not been interned in the current revision, so no references to\n            // it can exist.\n            let old_fields = unsafe { &mut *value.fields.get() };\n\n            // Remove the previous value from the ID map.\n            //\n            // Note that while the ID stays the same when a slot is reused, the fields,\n            // and thus the hash, will change, so we need to re-insert the value into the\n            // map. Crucially, we know that the hashes for the old and new fields both map\n            // to the same shard, because we determined the initial shard based on the new\n            // fields and only accessed the LRU list for that shard.\n            let old_hash = self.hasher.hash_one(&*old_fields);\n            shard\n                .key_map\n                .find_entry(old_hash, |found_id: &Id| *found_id == old_id)\n                .expect(\"interned value in LRU so must be in key_map\")\n                .remove();\n\n            // Update the fields.\n            //\n            // SAFETY: We call `from_internal_data` to restore the correct lifetime before access.\n            *old_fields = unsafe { self.to_internal_data(assemble(new_id, key)) };\n\n            // SAFETY: We hold the lock for the shard containing the value.\n            let hasher = |id: &_| unsafe { self.value_hash(*id, zalsa) };\n\n            // Insert the new value into the ID map.\n            shard.key_map.insert_unique(hash, new_id, hasher);\n\n            // Free the memos associated with the previous interned value.\n            //\n            // SAFETY: We hold the lock for the shard containing the value, and the\n            // value has not been interned in the current revision, so no references to\n            // it can exist.\n            let mut memo_table = unsafe { std::mem::take(&mut *value.memos.get()) };\n\n            // SAFETY: The memo table belongs to a value that we allocated, so it has the\n            // correct type.\n            unsafe { self.clear_memos(zalsa, &mut memo_table, new_id) };\n\n            if value_shared.is_reusable::<C>() {\n                // Move the value to the front of the LRU list.\n                //\n                // SAFETY: The value pointer is valid for the lifetime of the database.\n                // and never accessed mutably directly.\n                shard.lru.push_front(unsafe { UnsafeRef::from_raw(value) });\n            }\n\n            return new_id;\n        }\n\n        // If we could not find any stale slots, we are forced to allocate a new one.\n        self.intern_id_cold(\n            db,\n            key,\n            zalsa,\n            zalsa_local,\n            assemble,\n            shard,\n            shard_index,\n            hash,\n        )\n    }"
}
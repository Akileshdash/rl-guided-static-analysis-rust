{
    "level": "Info",
    "analyzer": "UnsafeDataflow",
    "op_type": "Transmute",
    "description": "Potential unsafe dataflow issue in `gemm::kernels::simd_generic::simd_int8_gemv`",
    "file": "rten-0.20.0/src/gemm/kernels/simd_generic.rs",
    "start_line": 749,
    "start_col": 1,
    "end_line": 960,
    "end_col": 2,
    "code_snippet": "pub fn simd_int8_gemv<I: Isa, const CAST_B_U8: bool>(\n    isa: I,\n    out: MatVecOutput<i32, bool>,\n    a: &[u8],\n    b: Matrix<i8>,\n    a_zero_point: u8,\n    b_zero_points: Option<&[i8]>,\n    dot: impl Int8DotProduct<X8 = I::I8, I32 = I::I32> + Copy,\n) {\n    // Verify that input and output dimensions are compatible.\n    assert_eq!(out.data.len(), b.cols());\n    assert_eq!(b.rows(), a.len());\n    assert_eq!(\n        b_zero_points.map(|zp| zp.len()).unwrap_or(b.cols()),\n        b.cols()\n    );\n\n    // Inner loop loads 4x u8 values at a time as an i32.\n    assert_eq!(a.as_ptr() as usize % align_of::<i32>(), 0);\n\n    if b.row_stride() == 1 {\n        // Safety: Input and output dimensions are compatible.\n        unsafe {\n            return simd_int8_gemv_transposed::<_, CAST_B_U8>(\n                isa,\n                out,\n                a,\n                b,\n                a_zero_point,\n                b_zero_points,\n                dot,\n            );\n        }\n    } else if b.col_stride() != 1 {\n        // Safety: Input and output dimensions are compatible.\n        unsafe {\n            return simd_int8_gemv_fallback::<CAST_B_U8>(out, a, b, a_zero_point, b_zero_points);\n        }\n    }\n\n    let ops = isa.i32();\n    let i8_ops = isa.i8();\n    let i16_ops = isa.i16();\n\n    let b_zero_shift = if CAST_B_U8 { 128 } else { 0 };\n    let bit_flip_mask = i8_ops.splat(I8_U8_SHIFT_MASK);\n\n    let a_ptr = a.as_ptr();\n    let depth = a.len();\n    let b_ptr = b.storage().as_ptr();\n    let b_row_stride = b.row_stride();\n\n    let row_sum: i32 = a.iter().map(|x| *x as i32).sum();\n\n    // Iterate over one SIMD vec of int8 input columns at a time, or 4x output\n    // i32 vecs.\n    let mut col_tiles = range_chunks_exact(0..b.cols(), i8_ops.len());\n    for col_tile in col_tiles.by_ref() {\n        let b_ptr = unsafe { b_ptr.add(col_tile.start) };\n        let mut acc = [ops.zero(); 4];\n        let mut col_sums = [ops.zero(); 4];\n        let one_u8 = i8_ops.splat(1);\n\n        // Loop over K tiles of size 4.\n        let mut k = 0;\n        while k + 4 <= depth {\n            // Broadcast 4 values from A.\n            let a_block = unsafe { *(a_ptr.add(k) as *const i32) };\n            let a = ops.splat(a_block).reinterpret_cast::<I::I8>();\n\n            // Load 4 rows of int8 elements from B and interleave to give 4\n            // transposed `[4, MR]` tiles. eg. Given 4 rows A, B, C, D if `MR` =\n            // 4, the first tile is stored in column-major order and contains:\n            //\n            // A0 A1 A2 A3\n            // B0 B1 B2 B3\n            // C0 C1 C2 C3\n            // D0 D1 D2 D3\n            //\n            // The second tile contains A4..A7 and so on.\n            let b_tile_ptr: [*const i8; 4] =\n                std::array::from_fn(|i| unsafe { b_ptr.add((k + i) * b_row_stride) });\n            let b0 = unsafe { i8_ops.load_ptr(b_tile_ptr[0]) };\n            let b1 = unsafe { i8_ops.load_ptr(b_tile_ptr[1]) };\n            let b2 = unsafe { i8_ops.load_ptr(b_tile_ptr[2]) };\n            let b3 = unsafe { i8_ops.load_ptr(b_tile_ptr[3]) };\n\n            let b01_lo = i8_ops.interleave_low(b0, b1).reinterpret_cast::<I::I16>();\n            let b01_hi = i8_ops.interleave_high(b0, b1).reinterpret_cast::<I::I16>();\n            let b23_lo = i8_ops.interleave_low(b2, b3).reinterpret_cast::<I::I16>();\n            let b23_hi = i8_ops.interleave_high(b2, b3).reinterpret_cast::<I::I16>();\n\n            let b_tiles = [\n                i16_ops.interleave_low(b01_lo, b23_lo),\n                i16_ops.interleave_high(b01_lo, b23_lo),\n                i16_ops.interleave_low(b01_hi, b23_hi),\n                i16_ops.interleave_high(b01_hi, b23_hi),\n            ]\n            .map(|t| t.reinterpret_cast::<I::I8>());\n\n            // Pre-fetch the current block of 4 rows for the next column tile.\n            for i in 0..4 {\n                i8_ops.prefetch(unsafe { b_tile_ptr[i].add(i8_ops.len()) });\n            }\n\n            for i in 0..4 {\n                let b_tile = if CAST_B_U8 {\n                    i8_ops.xor(b_tiles[i], bit_flip_mask)\n                } else {\n                    b_tiles[i]\n                };\n                acc[i] = dot.dot_product(a, b_tile, acc[i]);\n                col_sums[i] = dot.dot_product(one_u8, b_tile, col_sums[i]);\n            }\n            k += 4;\n        }\n\n        while k < depth {\n            let a_block = unsafe { (*a_ptr.add(k)).into() };\n            let a = ops.splat(a_block);\n\n            // Load one `i8` vec, sign-extend each quarter to give 4 `i32` vecs.\n            let b = unsafe { i8_ops.load_ptr(b_ptr.add(k * b_row_stride)) };\n            let (b01, b23) = i8_ops.extend(b);\n            let (b0, b1) = i16_ops.extend(b01);\n            let (b2, b3) = i16_ops.extend(b23);\n            let b_rows = [b0, b1, b2, b3];\n\n            for i in 0..4 {\n                let b = b_rows[i];\n                let b = if CAST_B_U8 {\n                    ops.add(b, ops.splat(b_zero_shift))\n                } else {\n                    b\n                };\n\n                acc[i] = ops.mul_add(a, b, acc[i]);\n                col_sums[i] = ops.add(col_sums[i], b);\n            }\n            k += 1;\n        }\n\n        // Subtract zero points. This is equivalent to doing\n        // `acc += (a - a_zero) * (b - b_zero)` in the loop over K, but more\n        // efficient.\n        let row_sum_vec = ops.splat(row_sum);\n        let depth_vec = ops.splat(depth as i32);\n        let a_zero_vec = ops.splat(a_zero_point.into());\n\n        let b_zero_vec = if let Some(b_zero) = b_zero_points {\n            // Load one `i8` vec, sign-extend each quarter to give 4 `i32` vecs.\n            let b = unsafe { i8_ops.load_ptr(b_zero.as_ptr().add(col_tile.start)) };\n            let (b01, b23) = i8_ops.extend(b);\n            let (b0, b1) = i16_ops.extend(b01);\n            let (b2, b3) = i16_ops.extend(b23);\n            [b0, b1, b2, b3]\n        } else {\n            [ops.zero(); 4]\n        };\n\n        for i in 0..4 {\n            let b_zero_vec = ops.add(b_zero_vec[i], ops.splat(b_zero_shift));\n\n            let tmp = ops.mul(depth_vec, a_zero_vec);\n            let tmp = ops.mul(tmp, b_zero_vec);\n            let tmp = ops.add(tmp, acc[i]);\n            let tmp = ops.sub(tmp, ops.mul(row_sum_vec, b_zero_vec));\n            acc[i] = ops.sub(tmp, ops.mul(col_sums[i], a_zero_vec));\n\n            let out_ptr =\n                unsafe { out.data.as_ptr().add(col_tile.start + i * ops.len()) as *mut i32 };\n            if !out.beta {\n                unsafe {\n                    ops.store_ptr(acc[i], out_ptr);\n                }\n            } else {\n                let tmp = unsafe { ops.load_ptr(out_ptr) };\n                let tmp = ops.add(tmp, acc[i]);\n                unsafe {\n                    ops.store_ptr(tmp, out_ptr);\n                }\n            }\n        }\n    }\n\n    for col in col_tiles.remainder() {\n        let mut acc = 0;\n        let mut col_sum = 0;\n        for (k, &a) in a.iter().enumerate() {\n            let mut b_val = unsafe { *b.get_unchecked([k, col]) as i32 };\n            if CAST_B_U8 {\n                b_val += b_zero_shift;\n            }\n            acc += a as i32 * b_val;\n            col_sum += b_val;\n        }\n\n        // Subtract zero points. This is equivalent to doing\n        // `acc += (a - a_zero) * (b - b_zero)` in the loop over K, but more\n        // efficient.\n        let a_zero = a_zero_point as i32;\n        let b_zero = b_zero_points.map(|bq| bq[col] as i32).unwrap_or(0) + b_zero_shift;\n        acc = depth as i32 * a_zero * b_zero + acc - row_sum * b_zero - col_sum * a_zero;\n\n        let out_el = unsafe { out.data.as_ptr().add(col) as *mut i32 };\n        if !out.beta {\n            unsafe { out_el.write(acc) };\n        } else {\n            unsafe { *out_el += acc };\n        }\n    }\n}"
}